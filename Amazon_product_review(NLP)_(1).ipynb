{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41dad4e5",
      "metadata": {
        "id": "41dad4e5"
      },
      "outputs": [],
      "source": [
        "#importing labraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "574c4836",
      "metadata": {
        "id": "574c4836"
      },
      "outputs": [],
      "source": [
        "#read dataset\n",
        "df= pd.read_csv(\"/content/Product_details.csv\")\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c513ce8",
      "metadata": {
        "id": "0c513ce8"
      },
      "outputs": [],
      "source": [
        "df['Product_Type'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a36c867",
      "metadata": {
        "id": "5a36c867"
      },
      "outputs": [],
      "source": [
        "df['Sentiment'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efcf8b0b",
      "metadata": {
        "id": "efcf8b0b"
      },
      "source": [
        "# Pre-Processing\n",
        "1.Expanding Contraction\n",
        "2.Language Detection\n",
        "3.Tokenization\n",
        "4.Converting all characters to lowercase\n",
        "5.Removing Punctuation\n",
        "6.Removing Stopwords\n",
        "7.Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f2a4886",
      "metadata": {
        "id": "6f2a4886"
      },
      "outputs": [],
      "source": [
        "#importing requied labraries\n",
        "import nltk\n",
        "!pip install contractions\n",
        "import contractions\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords , wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "pd.set_option('display.max_colwidth',100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07328a64",
      "metadata": {
        "id": "07328a64"
      },
      "outputs": [],
      "source": [
        "# drop Text_ID column\n",
        "df=df.drop('Text_ID',axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae369b80",
      "metadata": {
        "id": "ae369b80"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d190c15b",
      "metadata": {
        "id": "d190c15b"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f862b398",
      "metadata": {
        "id": "f862b398"
      },
      "outputs": [],
      "source": [
        "# checking for null values\n",
        "for col in df.columns:\n",
        "    print(col,df[col].isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61fdf480",
      "metadata": {
        "id": "61fdf480"
      },
      "outputs": [],
      "source": [
        "#Expanding Contractions\n",
        "df['no_contract']=df['Product_Description'].apply(lambda x: [contractions.fix(word) for word in x.split()])\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a16ddf55",
      "metadata": {
        "id": "a16ddf55"
      },
      "outputs": [],
      "source": [
        "df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b593d93",
      "metadata": {
        "id": "6b593d93"
      },
      "outputs": [],
      "source": [
        "#converting back to string\n",
        "df['Product_Description_str']=[' '.join(map(str,l)) for l in df['no_contract']]\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f764b3a5",
      "metadata": {
        "id": "f764b3a5"
      },
      "outputs": [],
      "source": [
        "#English Language Detection\n",
        "!pip install langdetect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82a0d8e0",
      "metadata": {
        "id": "82a0d8e0"
      },
      "outputs": [],
      "source": [
        "from langdetect import detect\n",
        "\n",
        "for sent in df['Product_Description_str']:\n",
        "     df['lang']=detect(sent)\n",
        "        \n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dcb5ca33",
      "metadata": {
        "id": "dcb5ca33"
      },
      "outputs": [],
      "source": [
        "df['lang'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e3df62c",
      "metadata": {
        "id": "6e3df62c"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "df['tokenized']=df['Product_Description_str'].apply(word_tokenize)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfc25a0e",
      "metadata": {
        "id": "cfc25a0e"
      },
      "outputs": [],
      "source": [
        "#Converting all the characters to Lowercase\n",
        "df['lower']=df['tokenized'].apply(lambda x:[word.lower() for word in x])\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a47cc54",
      "metadata": {
        "id": "7a47cc54"
      },
      "outputs": [],
      "source": [
        "#Removing Punctutions\n",
        "punc= string.punctuation\n",
        "df['no_punc']=df['lower'].apply(lambda x: [word for word in x if word not in punc])\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8da63cf7",
      "metadata": {
        "id": "8da63cf7"
      },
      "outputs": [],
      "source": [
        "#Removing Stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "stop_words=set(stopwords.words('english'))\n",
        "df['no_stopword']=df['no_punc'].apply(lambda x: [word for word in x if word not in stop_words])\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9363dbbf",
      "metadata": {
        "id": "9363dbbf"
      },
      "outputs": [],
      "source": [
        "#Lemmatization\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "wnl=WordNetLemmatizer()\n",
        "df['lemmatized']=df['no_stopword'].apply(lambda x:[wnl.lemmatize(word) for word in x])\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41990090",
      "metadata": {
        "id": "41990090"
      },
      "source": [
        "# EDA(Exploratory Data Analysis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7990e9f6",
      "metadata": {
        "id": "7990e9f6"
      },
      "outputs": [],
      "source": [
        "!pip install pyLDAvis\n",
        "import pyLDAvis.sklearn\n",
        "from collections import Counter\n",
        "from textblob import TextBlob\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation, NMF \n",
        "from wordcloud import WordCloud, ImageColorGenerator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4db7e28a",
      "metadata": {
        "id": "4db7e28a"
      },
      "outputs": [],
      "source": [
        "#converting back to string\n",
        "df['lemmatized_str']=[' '.join(map(str,l)) for l in df['lemmatized']]\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a36ecf69",
      "metadata": {
        "id": "a36ecf69"
      },
      "outputs": [],
      "source": [
        "df1=df[['Product_Type','Sentiment','lemmatized_str','lemmatized']]\n",
        "df1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c2a2d01",
      "metadata": {
        "scrolled": false,
        "id": "6c2a2d01"
      },
      "outputs": [],
      "source": [
        "#sentiment polarity analysis\n",
        "df1['sentiment_polarity']=df1['lemmatized_str'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
        "df1.head(20)"
      ]
    },
    {
      "cell_type": "raw",
      "id": "9078a8dc",
      "metadata": {
        "id": "9078a8dc"
      },
      "source": [
        "observation:-\n",
        "    1.for sentiment=2 there is a negative and positive polarity \n",
        "    2.for sentiment=1 the prolarity is 1,+ve and also -ve.\n",
        "    3.with above observation we can't say whether sentiment=1&2 contains positive words or negative. then it may neutral or mix of +ve& -ve sentiment.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98ce212c",
      "metadata": {
        "id": "98ce212c"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_row',None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e60c5c28",
      "metadata": {
        "id": "e60c5c28"
      },
      "outputs": [],
      "source": [
        "df1['sentiment_polarity'].groupby(df1['Sentiment']).value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab6a56ef",
      "metadata": {
        "id": "ab6a56ef"
      },
      "source": [
        "observation:-Each Sentiment has both Positive and Negative Polarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1c1862e",
      "metadata": {
        "id": "a1c1862e"
      },
      "outputs": [],
      "source": [
        "df1['sentiment_polarity'].groupby(df1['Product_Type']).value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ffce0840",
      "metadata": {
        "id": "ffce0840"
      },
      "source": [
        "Only product 4 has positive sentiment polarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "671c75bc",
      "metadata": {
        "id": "671c75bc"
      },
      "outputs": [],
      "source": [
        "#for the sentiment polarity we will plot a histogram and observe the distribution.\n",
        "plt.figure(figsize=(30,20))\n",
        "plt.xlabel('Sentiment Polarity',fontsize=50)\n",
        "plt.xticks(fontsize=40)\n",
        "plt.ylabel('Frequency',fontsize=20)\n",
        "plt.yticks(fontsize=40)\n",
        "plt.hist(df1['sentiment_polarity'],bins=50)\n",
        "plt.title('Sentiment Distribution',fontsize=60)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cff45d6f",
      "metadata": {
        "id": "cff45d6f"
      },
      "source": [
        "Most of the Distribution lies on right side of 0.00 (i.e,Positive).So,overall we can conclude that the customers are happy with products"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f64d3142",
      "metadata": {
        "id": "f64d3142"
      },
      "outputs": [],
      "source": [
        "x=df1.Sentiment.value_counts()\n",
        "y=x.sort_index()\n",
        "plt.figure(figsize=(50,30))\n",
        "sns.barplot(x.index,x.values,alpha=0.8)\n",
        "plt.title(\"Sentiment Distribution\",fontsize=40)\n",
        "plt.ylabel('Frequency',fontsize=40)\n",
        "plt.yticks(fontsize=40)\n",
        "plt.xlabel('Sentiment',fontsize=40)\n",
        "plt.xticks(fontsize=40)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18e909d9",
      "metadata": {
        "id": "18e909d9"
      },
      "outputs": [],
      "source": [
        "df1[df1['Sentiment']==2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5289873a",
      "metadata": {
        "id": "5289873a"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(30,10))\n",
        "plt.title('Percentage of Ratings',fontsize=20)\n",
        "df1.Sentiment.value_counts().plot(kind='pie',labels=['Sentiment2','Sentiment3','Sentiment1','Sentiment0'],\n",
        "                                 wedgeprops=dict(width=.7),autopct='%1.0f%%',startangle=-20,\n",
        "                                 textprops={'fontsize':15})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "548f92a2",
      "metadata": {
        "id": "548f92a2"
      },
      "outputs": [],
      "source": [
        "df1.groupby('Sentiment')['sentiment_polarity'].mean().plot(kind='bar',figsize=(50,30))\n",
        "plt.xlabel(\"Sentiment\",fontsize=40)\n",
        "plt.ylabel(\"Average Sentiment Polarity\",fontsize=40)\n",
        "plt.xticks(fontsize=40)\n",
        "plt.yticks(fontsize=40)\n",
        "plt.title('Avg. Sentiment polarity per Sentiment',fontsize=50)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3a97fc1",
      "metadata": {
        "id": "a3a97fc1"
      },
      "outputs": [],
      "source": [
        "df1['word_count']=df1['lemmatized'].apply(lambda x: len(str(x).split()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d15e747a",
      "metadata": {
        "id": "d15e747a"
      },
      "outputs": [],
      "source": [
        "df1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1248bf03",
      "metadata": {
        "id": "1248bf03"
      },
      "outputs": [],
      "source": [
        "df1['word_count'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e36c9d84",
      "metadata": {
        "id": "e36c9d84"
      },
      "source": [
        "Let's observe that the longest review is for negative or neutral.this can be done by finding correlation matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fae02a6",
      "metadata": {
        "id": "7fae02a6"
      },
      "outputs": [],
      "source": [
        "df1.groupby('Sentiment')['word_count'].mean().plot(kind='bar',figsize=(50,30))\n",
        "plt.xlabel('Sentiment',fontsize=40)\n",
        "plt.ylabel('count of words',fontsize=40)\n",
        "plt.xticks(fontsize=40)\n",
        "plt.yticks(fontsize=40)\n",
        "plt.title('Average No. of words per Sentiment Distribution',fontsize=40)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "396e45a3",
      "metadata": {
        "id": "396e45a3"
      },
      "source": [
        "Sentiment 1 contains longest reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd748b0a",
      "metadata": {
        "id": "dd748b0a"
      },
      "outputs": [],
      "source": [
        "df1[df1['Sentiment']==1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d793a4b7",
      "metadata": {
        "id": "d793a4b7"
      },
      "outputs": [],
      "source": [
        "#correlation matrix\n",
        "correlation=df1[['Sentiment','sentiment_polarity','word_count']].corr()\n",
        "mask=np.zeros_like(correlation,dtype=np.bool)\n",
        "mask[np.triu_indices_from(mask)]=True\n",
        "plt.figure(figsize=(50,30))\n",
        "plt.xticks(fontsize=40)\n",
        "plt.yticks(fontsize=40)\n",
        "sns.heatmap(correlation,cmap='coolwarm',annot=True,annot_kws={\"size\":40},linewidths=10,vmin=-1.5,mask=mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16deccdd",
      "metadata": {
        "id": "16deccdd"
      },
      "source": [
        "word_count and sentiment are negatively correlated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c64b7357",
      "metadata": {
        "id": "c64b7357"
      },
      "outputs": [],
      "source": [
        "words= df1['lemmatized']\n",
        "allwords=[]\n",
        "for wordlist in words:\n",
        "    allwords +=wordlist\n",
        "    \n",
        "print(len(allwords))    \n",
        "print(allwords)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb0a8101",
      "metadata": {
        "id": "eb0a8101"
      },
      "outputs": [],
      "source": [
        "#wordcloud for top 100 most common words\n",
        "mostcommon=FreqDist(allwords).most_common(100)\n",
        "\n",
        "wordcloud=WordCloud(width=1600,height=800,background_color='white').generate(str(mostcommon))\n",
        "fig=plt.figure(figsize=(40,15),facecolor='white')\n",
        "plt.imshow(wordcloud,interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Top 100 Most Common Words',fontsize=100)\n",
        "\n",
        "plt.tight_layout(pad=0)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c45a8782",
      "metadata": {
        "id": "c45a8782"
      },
      "source": [
        "this frequency analysis cenrtainly supports the overall positive sentiment of the reviews.\n",
        "Terms such as good,love,awesome are showing that the customers are enjoying the products.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "645a57c1",
      "metadata": {
        "id": "645a57c1"
      },
      "outputs": [],
      "source": [
        "#most frequent words for sentiment0.\n",
        "#while interpreting the result for sentiment0 we have to be careful as it contribute only 2%(as shown is pie chart above)\n",
        "group_by=df1.groupby('Sentiment')['lemmatized_str'].apply(lambda x: Counter(' '.join(x).split()).most_common(25))\n",
        "\n",
        "#for sentiment0\n",
        "group_by_0=group_by.iloc[0]\n",
        "word0=list(zip(*group_by_0))[0]\n",
        "freq0=list(zip(*group_by_0))[1]\n",
        "\n",
        "plt.figure(figsize=(50,30))\n",
        "plt.bar(word0,freq0)\n",
        "plt.xlabel('Words',fontsize=50)\n",
        "plt.ylabel('Frequancy of words',fontsize=50)\n",
        "plt.yticks(fontsize=40)\n",
        "plt.xticks(rotation=60,fontsize=40)\n",
        "plt.title('Frequency of 25 words for sentiment0',fontsize=60)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65146c66",
      "metadata": {
        "id": "65146c66"
      },
      "source": [
        "we can remove the words such as sxsw, mention,google,link as they accure very frequenly.Also it is very difficult to drive insights from neutral words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9893558f",
      "metadata": {
        "id": "9893558f"
      },
      "outputs": [],
      "source": [
        "#for sentiment1\n",
        "group_by_1=group_by.iloc[1]\n",
        "word0=list(zip(*group_by_0))[0]\n",
        "freq0=list(zip(*group_by_0))[1]\n",
        "\n",
        "plt.figure(figsize=(50,30))\n",
        "plt.bar(word0,freq0)\n",
        "plt.xlabel('Words',fontsize=50)\n",
        "plt.ylabel('Frequancy of words',fontsize=50)\n",
        "plt.yticks(fontsize=40)\n",
        "plt.xticks(rotation=60,fontsize=40)\n",
        "plt.title('Frequency of 25 words for sentiment1',fontsize=60)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c09c1ad5",
      "metadata": {
        "id": "c09c1ad5"
      },
      "source": [
        "sentiment1 also don't have clues for product improvement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c8790ba",
      "metadata": {
        "id": "4c8790ba"
      },
      "outputs": [],
      "source": [
        "#for sentiment2\n",
        "group_by_2=group_by.iloc[2]\n",
        "word0=list(zip(*group_by_0))[0]\n",
        "freq0=list(zip(*group_by_0))[1]\n",
        "\n",
        "plt.figure(figsize=(50,30))\n",
        "plt.bar(word0,freq0)\n",
        "plt.xlabel('Words',fontsize=50)\n",
        "plt.ylabel('Frequancy of words',fontsize=50)\n",
        "plt.yticks(fontsize=40)\n",
        "plt.xticks(rotation=60,fontsize=40)\n",
        "plt.title('Frequency of 25 words for sentiment2',fontsize=60)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03e3bc48",
      "metadata": {
        "id": "03e3bc48"
      },
      "outputs": [],
      "source": [
        "group_by_3=group_by.iloc[3]\n",
        "word0=list(zip(*group_by_0))[0]\n",
        "freq0=list(zip(*group_by_0))[1]\n",
        "\n",
        "plt.figure(figsize=(50,30))\n",
        "plt.bar(word0,freq0)\n",
        "plt.xlabel('Words',fontsize=50)\n",
        "plt.ylabel('Frequancy of words',fontsize=50)\n",
        "plt.yticks(fontsize=40)\n",
        "plt.xticks(rotation=60,fontsize=40)\n",
        "plt.title('Frequency of 25 words for sentiment3',fontsize=60)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "5n0kTW3_k5jy"
      },
      "id": "5n0kTW3_k5jy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b357e06b",
      "metadata": {
        "id": "b357e06b"
      },
      "source": [
        "same 25 words are repeating for all the sentiments.there is no such words which shows positiveness. therefore we can't conclude anything from these words.it is better to remove them all."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0fa5174",
      "metadata": {
        "id": "d0fa5174"
      },
      "source": [
        "# Topic Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2a22943",
      "metadata": {
        "id": "c2a22943"
      },
      "source": [
        "# Count Vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1d7c8e9",
      "metadata": {
        "id": "f1d7c8e9"
      },
      "outputs": [],
      "source": [
        "df2=df1[['Sentiment','Product_Type','sentiment_polarity']]\n",
        "df2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "962691fb",
      "metadata": {
        "scrolled": true,
        "id": "962691fb"
      },
      "outputs": [],
      "source": [
        "#Count Vectorizer\n",
        "tf_vectorizer = CountVectorizer(max_df=0.9,min_df=25,max_features=6000)\n",
        "tf=tf_vectorizer.fit_transform(df1['lemmatized_str'].values.astype('U'))\n",
        "tf_feature_names = tf_vectorizer.get_feature_names()\n",
        "\n",
        "df3 = pd.DataFrame(tf.toarray(),columns=list(tf_feature_names))\n",
        "df3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2153ef81",
      "metadata": {
        "id": "2153ef81"
      },
      "outputs": [],
      "source": [
        "d=[df2,df3]\n",
        "data=pd.concat(d,axis=1)\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da951b3e",
      "metadata": {
        "id": "da951b3e"
      },
      "outputs": [],
      "source": [
        "x=data.drop('Sentiment',axis=1)\n",
        "y=data['Sentiment']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23b95353",
      "metadata": {
        "id": "23b95353"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae24167c",
      "metadata": {
        "id": "ae24167c"
      },
      "source": [
        "# Model Building(Naive Bayes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11097af3",
      "metadata": {
        "id": "11097af3"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "classifier = GaussianNB()\n",
        "classifier.fit(x_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73a9a41d",
      "metadata": {
        "id": "73a9a41d"
      },
      "outputs": [],
      "source": [
        "y_pred=classifier.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7407e931",
      "metadata": {
        "id": "7407e931"
      },
      "outputs": [],
      "source": [
        "#Accuracy Check\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score\n",
        "cm=confusion_matrix(y_test,y_pred)\n",
        "print(cm)\n",
        "\n",
        "accuracy_score(y_test,y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7eff495e",
      "metadata": {
        "id": "7eff495e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f87597ac",
      "metadata": {
        "id": "f87597ac"
      },
      "outputs": [],
      "source": [
        "#without product type and sentiment polarity\n",
        "df2=df1['Sentiment']\n",
        "d=[df2,df3]\n",
        "data1=pd.concat(d,axis=1)\n",
        "data1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d73e3d4",
      "metadata": {
        "id": "9d73e3d4"
      },
      "outputs": [],
      "source": [
        "X=data1.drop('Sentiment',axis=1)\n",
        "Y=data1['Sentiment']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e1fd2ee",
      "metadata": {
        "id": "0e1fd2ee"
      },
      "outputs": [],
      "source": [
        "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.3,random_state=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78dabb22",
      "metadata": {
        "id": "78dabb22"
      },
      "outputs": [],
      "source": [
        "classifier.fit(X_train,Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2a0acfb",
      "metadata": {
        "id": "e2a0acfb"
      },
      "outputs": [],
      "source": [
        "Y_pred=classifier.predict(X_test)\n",
        "accuracy_score(Y_test,Y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58e7f18c",
      "metadata": {
        "id": "58e7f18c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04b73ff3",
      "metadata": {
        "id": "04b73ff3"
      },
      "outputs": [],
      "source": [
        "#Random Forest\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "clf=RandomForestClassifier(n_estimators=100)\n",
        "clf.fit(x_train,y_train)\n",
        "y_pred=clf.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3eda250c",
      "metadata": {
        "id": "3eda250c"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "print('Accuracy:',metrics.accuracy_score(y_test,y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xhJ1H_PkhJd1"
      },
      "id": "xhJ1H_PkhJd1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abfc44c8",
      "metadata": {
        "id": "abfc44c8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48c919a3",
      "metadata": {
        "id": "48c919a3"
      },
      "outputs": [],
      "source": [
        "#Random Forest\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "clf=RandomForestClassifier(n_estimators=100)\n",
        "clf.fit(X_train,Y_train)\n",
        "Y_pred=clf.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26a4922e",
      "metadata": {
        "id": "26a4922e"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "print('Accuracy:',metrics.accuracy_score(Y_test,Y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c1ec74f",
      "metadata": {
        "id": "1c1ec74f"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "y_true = [0, 1, 2, 2, 2]\n",
        "y_pred = [0, 0, 2, 2, 1]\n",
        "target_names = ['class 0', 'class 1', 'class 2']\n",
        "print(classification_report(y_true, y_pred, target_names=target_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e845e9c",
      "metadata": {
        "id": "5e845e9c"
      },
      "outputs": [],
      "source": [
        "#TF-IDF\n",
        "#instead of using count vectorizer, now we will use TF-IDF. this method help to bring down the weight of high frequency words.\n",
        "tfidf_vectorizer = TfidfVectorizer(max_df=0.90,min_df=25,max_features=6000,use_idf=True)\n",
        "\n",
        "tfidf=tfidf_vectorizer.fit_transform(df1['lemmatized_str'])\n",
        "tfidf_feature_names=tfidf_vectorizer.get_feature_names()\n",
        "\n",
        "df3=pd.DataFrame(tfidf.toarray(),columns=list(tfidf_feature_names))\n",
        "df3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "349ec588",
      "metadata": {
        "id": "349ec588"
      },
      "outputs": [],
      "source": [
        "d=[df2,df3]\n",
        "data1=pd.concat(d,axis=1)\n",
        "data1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4029c944",
      "metadata": {
        "id": "4029c944"
      },
      "outputs": [],
      "source": [
        "X=data1.drop('Sentiment',axis=1)\n",
        "Y=data1['Sentiment']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "deead9c9",
      "metadata": {
        "id": "deead9c9"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.3,random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04d56fd0",
      "metadata": {
        "id": "04d56fd0"
      },
      "outputs": [],
      "source": [
        "#Naive Bayes model\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "classifier=GaussianNB()\n",
        "classifier.fit(X_train,Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3da51389",
      "metadata": {
        "id": "3da51389"
      },
      "outputs": [],
      "source": [
        "#predict and accuracy\n",
        "y_pred1=classifier.predict(X_test)\n",
        "\n",
        "#Accuracy\n",
        "accuracy_score(Y_test,y_pred1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Random Forest\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "clf=RandomForestClassifier(n_estimators=100)\n",
        "clf.fit(x_train,y_train)\n",
        "y_pred=clf.predict(x_test)"
      ],
      "metadata": {
        "id": "xdvQDExqfT0F"
      },
      "id": "xdvQDExqfT0F",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "print('Accuracy:',metrics.accuracy_score(y_test,y_pred))"
      ],
      "metadata": {
        "id": "dms1MgdMhLL1"
      },
      "id": "dms1MgdMhLL1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "y_true = [0, 1, 2, 2, 2]\n",
        "y_pred = [0, 0, 2, 2, 1]\n",
        "target_names = ['class 0', 'class 1', 'class 2']\n",
        "print(classification_report(y_true, y_pred, target_names=target_names))"
      ],
      "metadata": {
        "id": "_A-v1-Dah1Sr"
      },
      "id": "_A-v1-Dah1Sr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = RandomForestClassifier"
      ],
      "metadata": {
        "id": "FB867RapiKt4"
      },
      "id": "FB867RapiKt4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model= RandomForestClassifier()\n",
        "model.fit(x_train, y_train)\n",
        "y_pred=model.predict(x_test)\n",
        "print(classification_report(y_test,y_pred))"
      ],
      "metadata": {
        "id": "VyiHaENpif_u"
      },
      "id": "VyiHaENpif_u",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "pickle.dump(model, open('rf.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "zjiz2_htjTXV"
      },
      "id": "zjiz2_htjTXV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xl308sutjwRG"
      },
      "id": "Xl308sutjwRG",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}